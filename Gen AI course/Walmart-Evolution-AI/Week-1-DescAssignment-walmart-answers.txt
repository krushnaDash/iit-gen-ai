------------------------------------------------------------------------------------------


COURSE: Advanced AI
TOPIC: Evolution of AI & Introduction to Transformer Models
DUE-DATE: 19 Jan 2025


INSTRUCTIONS:
1. Do not modify any tag in this file. Just type your answers in the designated place.
2. Do not change the file name and/or extension.
3. Upload this file after answering in the portal.


------------------------------------------------------------------------------------------


<<< QUESTION 1 >>>


What is Turing test? How is it performed? 


### WRITE ANSWER BELOW ###

The Turing Test is a measure of machine intelligence proposed by Alan Turing in 1950 in his paper "Computing Machinery and Intelligence." It evaluates whether a machine can exhibit intelligent behavior indistinguishable from a human.

How it is performed:
1. Setup: The test involves three participants - a human evaluator (judge), a human respondent, and a machine (computer).

2. Isolation: The evaluator is separated from both the human and the machine, communicating only through text-based messages (to avoid voice or appearance bias).

3. Conversation: The evaluator engages in natural language conversations with both participants without knowing which is the machine and which is the human.

4. Evaluation: The evaluator asks questions and receives responses from both participants. The questions can be on any topic - reasoning, emotions, general knowledge, etc.

5. Judgment: After the conversation, the evaluator tries to determine which participant is the machine and which is the human.

6. Result: If the evaluator cannot reliably distinguish the machine from the human (typically if the machine fools the evaluator more than 30% of the time), the machine is said to have passed the Turing Test.

The test focuses on behavioral intelligence rather than internal processes, meaning it tests whether a machine can "act" intelligently rather than whether it truly "thinks."




<<< QUESTION 2 >>>


What is an expert system? Explain its characteristics and objectives.


### WRITE ANSWER BELOW ###

An Expert System is a computer program that emulates the decision-making ability of a human expert in a specific domain. It uses knowledge bases and inference engines to solve complex problems that typically require human expertise.

Characteristics of Expert Systems:

1. Knowledge Base: Contains domain-specific facts, rules, and heuristics gathered from human experts. This is the core repository of expertise.

2. Inference Engine: The reasoning mechanism that applies logical rules to the knowledge base to derive conclusions and make decisions.

3. Explanation Facility: Can explain its reasoning process and justify its conclusions, making the decision-making transparent to users.

4. User Interface: Provides an interactive way for users to input queries and receive expert advice in an understandable format.

5. Knowledge Acquisition Module: Allows the system to be updated with new knowledge and rules as the domain evolves.

6. Domain Specificity: Designed for narrow, well-defined problem domains rather than general intelligence.

7. Uncertainty Handling: Can work with incomplete or uncertain information using probabilistic reasoning.

Objectives of Expert Systems:

1. Preserve Expertise: Capture and preserve valuable human expertise that might otherwise be lost.

2. Democratize Knowledge: Make expert-level advice available to non-experts and in locations where human experts are unavailable.

3. Consistency: Provide consistent recommendations without fatigue, emotions, or bias.

4. Training Tool: Serve as educational tools to train new professionals in a domain.

5. Decision Support: Assist human experts in making faster and more accurate decisions.

6. Cost Reduction: Reduce dependency on expensive human experts for routine consultations.

Examples include MYCIN (medical diagnosis), DENDRAL (chemical analysis), and XCON (computer configuration).




<<< QUESTION 3 >>>


Why do modern generative models sometimes produce hallucinations?


### WRITE ANSWER BELOW ###

Hallucinations in generative AI refer to instances where models generate content that is factually incorrect, nonsensical, or not grounded in the input data. Several factors contribute to this phenomenon:

1. Statistical Pattern Learning:
   - Generative models learn statistical patterns from training data rather than true understanding.
   - They predict the most probable next token based on patterns, not factual accuracy.
   - This can lead to plausible-sounding but incorrect outputs.

2. Training Data Limitations:
   - Models may have incomplete, outdated, or biased training data.
   - Knowledge cutoff dates mean models lack information about recent events.
   - Contradictory information in training data can confuse the model.

3. Lack of Grounding:
   - Models don't have access to real-time information or external verification.
   - They cannot fact-check their outputs against authoritative sources.
   - No mechanism exists to distinguish between factual and fictional content.

4. Overconfidence in Generation:
   - Models are trained to always produce an output, even when uncertain.
   - They lack the ability to say "I don't know" reliably.
   - The autoregressive nature forces continuation even when the model is uncertain.

5. Context Window Limitations:
   - Limited context windows can cause models to lose track of earlier information.
   - Long documents may have inconsistencies due to attention limitations.

6. Interpolation and Extrapolation:
   - When faced with queries outside training distribution, models interpolate or extrapolate.
   - This creative filling of gaps can produce fabricated information.

7. Optimization Objective Mismatch:
   - Models are optimized for fluency and coherence, not factual accuracy.
   - A well-structured false statement may score higher than an awkward true one.

Mitigation strategies include retrieval-augmented generation (RAG), fine-tuning with factual data, and implementing verification mechanisms.




<<< QUESTION 4 >>>


What are the building blocks of encoder in transformer and what roles do they play? 


### WRITE ANSWER BELOW ###

The Transformer encoder consists of a stack of identical layers (typically 6 in the original architecture). Each encoder layer has the following building blocks:

1. Input Embedding Layer:
   - Converts input tokens into dense vector representations.
   - Maps discrete tokens to continuous vector space of dimension d_model.
   - Enables the model to work with numerical representations of text.

2. Positional Encoding:
   - Adds position information to embeddings since transformers lack inherent sequence order.
   - Uses sine and cosine functions of different frequencies.
   - Allows the model to understand the relative and absolute positions of tokens.

3. Multi-Head Self-Attention:
   - The core mechanism that allows each token to attend to all other tokens in the sequence.
   - Computes attention weights using Query (Q), Key (K), and Value (V) matrices.
   - Multiple attention heads capture different types of relationships simultaneously.
   - Role: Captures contextual dependencies and relationships between all tokens regardless of distance.

4. Add & Norm (Residual Connection + Layer Normalization):
   - Residual Connection: Adds the input of a sub-layer to its output (skip connection).
   - Layer Normalization: Normalizes the activations to stabilize training.
   - Role: Helps with gradient flow, enables training of deeper networks, and speeds up convergence.

5. Feed-Forward Network (FFN):
   - A position-wise fully connected network applied to each position independently.
   - Typically consists of two linear transformations with a ReLU activation in between.
   - Formula: FFN(x) = max(0, xW1 + b1)W2 + b2
   - Role: Adds non-linearity and transforms the representations, allowing the model to learn complex patterns.

6. Another Add & Norm Layer:
   - Applied after the feed-forward network.
   - Same role as above: stabilizes training and enables residual learning.

The encoder processes the entire input sequence in parallel and produces contextualized representations that capture the meaning of each token in relation to all other tokens.




<<< QUESTION 5 >>>


Explain multi-head attention and its advantages over single-head attention.


### WRITE ANSWER BELOW ###

Multi-Head Attention is an extension of the self-attention mechanism where multiple attention operations are performed in parallel, each with different learned linear projections.

How Multi-Head Attention Works:

1. Linear Projections: The input is projected into multiple sets of Query (Q), Key (K), and Value (V) matrices using different learned weight matrices for each head.

2. Parallel Attention: Each head independently computes scaled dot-product attention:
   Attention(Q, K, V) = softmax(QK^T / √d_k) × V

3. Concatenation: The outputs from all heads are concatenated together.

4. Final Projection: The concatenated output is passed through a final linear layer to produce the output.

Formula:
MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) × W_O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

Advantages over Single-Head Attention:

1. Multiple Representation Subspaces:
   - Each head can learn different types of relationships (syntactic, semantic, positional).
   - Single-head attention is limited to one type of attention pattern.

2. Richer Feature Learning:
   - Different heads can focus on different aspects: one on local context, another on long-range dependencies.
   - Captures diverse linguistic patterns simultaneously.

3. Improved Expressiveness:
   - Multiple heads provide more parameters and flexibility.
   - Can model complex relationships that single attention cannot capture.

4. Robustness:
   - If one head learns a suboptimal pattern, others can compensate.
   - Reduces the risk of the model focusing on only one type of pattern.

5. Parallel Computation:
   - All heads compute simultaneously, maintaining computational efficiency.
   - No sequential dependency between heads.

6. Better Gradient Flow:
   - Multiple pathways for gradients to flow during backpropagation.
   - Helps in training stability.

Example: In the sentence "The cat sat on the mat because it was tired," different heads might:
- Head 1: Focus on "it" → "cat" (coreference)
- Head 2: Focus on "sat" → "on the mat" (spatial relationship)
- Head 3: Focus on "tired" → "cat" (attribute relationship)




<<< QUESTION 6 >>>


What is self-attention? Explain how does it work using Query, Key, and Value. 


### WRITE ANSWER BELOW ###

Self-Attention (also called intra-attention) is a mechanism that allows each element in a sequence to attend to all other elements in the same sequence, computing a weighted representation based on relevance.

Core Concept:
Self-attention enables the model to understand context by determining how much focus each word should place on every other word in the sequence when encoding a particular word.

Query, Key, and Value Explained:

1. Query (Q): Represents "what am I looking for?"
   - The current token's representation used to search for relevant information.
   - Think of it as a question being asked.

2. Key (K): Represents "what do I contain?"
   - Each token's representation that describes its content.
   - Think of it as labels or tags that can be matched against queries.

3. Value (V): Represents "what information do I provide?"
   - The actual content/information that will be retrieved.
   - Think of it as the data returned when a query matches a key.

How Self-Attention Works (Step-by-Step):

Step 1: Create Q, K, V Matrices
- For each input token embedding (x), compute:
  Q = x × W_Q
  K = x × W_K  
  V = x × W_V
- W_Q, W_K, W_V are learned weight matrices.

Step 2: Calculate Attention Scores
- Compute dot product between Query and all Keys:
  Score = Q × K^T
- This measures similarity/relevance between tokens.

Step 3: Scale the Scores
- Divide by √d_k (square root of key dimension):
  Scaled Score = Score / √d_k
- Prevents dot products from becoming too large, which would push softmax into regions with tiny gradients.

Step 4: Apply Softmax
- Convert scores to probabilities (attention weights):
  Attention Weights = softmax(Scaled Score)
- Weights sum to 1, indicating relative importance.

Step 5: Compute Weighted Sum
- Multiply attention weights with Values:
  Output = Attention Weights × V
- Each token's output is a weighted combination of all Values.

Complete Formula:
Attention(Q, K, V) = softmax(QK^T / √d_k) × V

Example:
Sentence: "The cat sat on the mat"

For the word "sat":
- Query of "sat" compares with Keys of all words.
- High attention score with "cat" (subject) and "mat" (location).
- Lower attention with "the" (less relevant).
- Output for "sat" is weighted combination emphasizing "cat" and "mat" contexts.

This allows "sat" to incorporate contextual information from relevant words, creating a rich, context-aware representation.
