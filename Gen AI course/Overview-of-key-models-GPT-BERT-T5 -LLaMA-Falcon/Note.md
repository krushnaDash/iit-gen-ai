



What is the additional number of parameters that you will need apart from the pretrained weights in the following scenarios? (a) Text classification (3 classes) using BERT-base, (b) Text Summarization using T5.





Consider a task where you would have a paragraph and a question as input, and you need to generate the answer. Which pretrained model would you use among BERT, GPT and T5? What are the advantages and disadvantages of the allowed architectures in terms of (a) number of parameters to be fine-tuned and (b) representation capacity in the context of this problem?  Can you think of a method that can take advantage of each of the allowed architectures? [Assume that all these models have the same model dimension and number of layers]


Show that the self-attention score in the rotatory embeddings depends on the positional difference between the query and key.

